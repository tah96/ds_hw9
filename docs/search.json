[
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "Modeling For Data Science",
    "section": "",
    "text": "We’ll first read in our libraries and our dataset of interest\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\nlibrary(parsnip)\nlibrary(lubridate)\nlibrary(see)\nlibrary(glmnet)\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoaded glmnet 4.1-8\n\nlibrary(baguette)\nlibrary(ranger)\nlibrary(rpart.plot)\n\nLoading required package: rpart\n\nAttaching package: 'rpart'\n\nThe following object is masked from 'package:dials':\n\n    prune\n\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\n\n\nbike_data &lt;- read_csv(file='https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv',show_col_types = FALSE)"
  },
  {
    "objectID": "models.html#reading-in-libraries-and-our-data",
    "href": "models.html#reading-in-libraries-and-our-data",
    "title": "Modeling For Data Science",
    "section": "",
    "text": "We’ll first read in our libraries and our dataset of interest\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\nlibrary(parsnip)\nlibrary(lubridate)\nlibrary(see)\nlibrary(glmnet)\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoaded glmnet 4.1-8\n\nlibrary(baguette)\nlibrary(ranger)\nlibrary(rpart.plot)\n\nLoading required package: rpart\n\nAttaching package: 'rpart'\n\nThe following object is masked from 'package:dials':\n\n    prune\n\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\n\n\nbike_data &lt;- read_csv(file='https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv',show_col_types = FALSE)"
  },
  {
    "objectID": "models.html#exploratory-data-analyses",
    "href": "models.html#exploratory-data-analyses",
    "title": "Modeling For Data Science",
    "section": "Exploratory Data Analyses",
    "text": "Exploratory Data Analyses\nBefore building our models, we want to get familiar with our data and perform some non-transformative data transformations if needed.\nLets check for the missing values in our data…The good news is there are no missing values!\n\ncolSums(is.na(bike_data))\n\n                        Date            Rented Bike Count \n                           0                            0 \n                        Hour           Temperature(\\xb0C) \n                           0                            0 \n                 Humidity(%)             Wind speed (m/s) \n                           0                            0 \n            Visibility (10m) Dew point temperature(\\xb0C) \n                           0                            0 \n     Solar Radiation (MJ/m2)                 Rainfall(mm) \n                           0                            0 \n               Snowfall (cm)                      Seasons \n                           0                            0 \n                     Holiday              Functioning Day \n                           0                            0 \n\n\nDiving a level further, lets make sure all of our numeric columns where we should only expect values greater than zero follow that pattern. We will also want to make sure our assumption on values for Seasons, Holiday and Functioning Day hold true.\nEverything seems to look good per the output below\n\nnumColsInterest &lt;- list(rented_bike &lt;- bike_data$`Rented Bike Count`,\n                        hour &lt;- bike_data$Hour,\n                        humid &lt;- bike_data$`Humidity(%)`,\n                        wind &lt;- bike_data$`Wind speed (m/s)`,\n                        vis &lt;- bike_data$`Visibility (10m)`,\n                        solar &lt;- bike_data$`Solar Radiation (MJ/m2)`,\n                        rain &lt;- bike_data$`Rainfall(mm)`,\n                        snow &lt;- bike_data$`Snowfall (cm)`\n                        \n)\n\ncatColsInterest &lt;- list(seasons_unique = unique(bike_data$Seasons),\n                        holiday_unique = unique(bike_data$Holiday),\n                        day_unique = unique(bike_data$`Functioning Day`))\n\ncatColsInterest\n\n$seasons_unique\n[1] \"Winter\" \"Spring\" \"Summer\" \"Autumn\"\n\n$holiday_unique\n[1] \"No Holiday\" \"Holiday\"   \n\n$day_unique\n[1] \"Yes\" \"No\" \n\nnumMins &lt;- lapply(numColsInterest,min)\nstr(numMins)\n\nList of 8\n $ : num 0\n $ : num 0\n $ : num 0\n $ : num 0\n $ : num 27\n $ : num 0\n $ : num 0\n $ : num 0\n\n\nWe want a series of transformations and renamings. We want the Date column in a data format. We want Seasons,Holiday,Functioning Day as factors. We also want to rename all of our columns so they’re easier to work with using camel_case format.\nWe can see everything reflected in our structure output.\n\nbike_data &lt;- bike_data %&gt;%\n  mutate(Date = dmy(Date),\n         Seasons = as.factor(Seasons),\n         Holiday = as.factor(Holiday),\n         `Functioning Day` = as.factor(`Functioning Day`)\n         ) %&gt;%\n  rename(date = Date,\n         rented_bike_count = `Rented Bike Count`,\n         hour = Hour,\n         temperature_c = `Temperature(\\xb0C)`,\n         humidity_perc = `Humidity(%)`,\n         wind_speed_ms = `Wind speed (m/s)`,\n         visibility = `Visibility (10m)`,\n         dew_temp = `Dew point temperature(\\xb0C)`,\n         solar_radiation = `Solar Radiation (MJ/m2)`,\n         rainfall_mm = `Rainfall(mm)`,\n         snowfall_cm = `Snowfall (cm)`,\n         season = Seasons,\n         holiday = Holiday,\n         func_day = `Functioning Day`\n         )\n\nstr(bike_data)\n\ntibble [8,760 × 14] (S3: tbl_df/tbl/data.frame)\n $ date             : Date[1:8760], format: \"2017-12-01\" \"2017-12-01\" ...\n $ rented_bike_count: num [1:8760] 254 204 173 107 78 100 181 460 930 490 ...\n $ hour             : num [1:8760] 0 1 2 3 4 5 6 7 8 9 ...\n $ temperature_c    : num [1:8760] -5.2 -5.5 -6 -6.2 -6 -6.4 -6.6 -7.4 -7.6 -6.5 ...\n $ humidity_perc    : num [1:8760] 37 38 39 40 36 37 35 38 37 27 ...\n $ wind_speed_ms    : num [1:8760] 2.2 0.8 1 0.9 2.3 1.5 1.3 0.9 1.1 0.5 ...\n $ visibility       : num [1:8760] 2000 2000 2000 2000 2000 ...\n $ dew_temp         : num [1:8760] -17.6 -17.6 -17.7 -17.6 -18.6 -18.7 -19.5 -19.3 -19.8 -22.4 ...\n $ solar_radiation  : num [1:8760] 0 0 0 0 0 0 0 0 0.01 0.23 ...\n $ rainfall_mm      : num [1:8760] 0 0 0 0 0 0 0 0 0 0 ...\n $ snowfall_cm      : num [1:8760] 0 0 0 0 0 0 0 0 0 0 ...\n $ season           : Factor w/ 4 levels \"Autumn\",\"Spring\",..: 4 4 4 4 4 4 4 4 4 4 ...\n $ holiday          : Factor w/ 2 levels \"Holiday\",\"No Holiday\": 2 2 2 2 2 2 2 2 2 2 ...\n $ func_day         : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n\n\nWe want to create some summary statistics. We want to look our our rented_bike_count across our categorical variables season, holiday and func_day.\n\nbike_summaries &lt;- list(general=NULL, season = NULL, holiday = NULL, func_day = NULL)\n\nsummarizeNumeric &lt;- function(data,catVar){\n  catSym &lt;- sym(catVar)\n  summary_data &lt;- bike_data %&gt;%\n    select(rented_bike_count,!!catSym) %&gt;%\n    group_by(!!catSym) %&gt;%\n    summarize(across(everything(), .fns = list(\"mean\" = mean,\n                                                 \"median\" = median,\n                                                 \"var\" = var,\n                                                 \"sd\" = sd,\n                                                 \"IQR\" = IQR), .names = \"{.fn}_{.col}\"))\n  return(summary_data)\n}\n\n\nbike_summaries$season &lt;- summarizeNumeric(bike_data,\"season\")\nbike_summaries$holiday &lt;- summarizeNumeric(bike_data,\"holiday\")\nbike_summaries$func_day &lt;- summarizeNumeric(bike_data,\"func_day\")\n\nbike_summaries$general &lt;- bike_data %&gt;%\n  select(rented_bike_count) %&gt;%\n  summarize(across(everything(), .fns = list(\"mean\" = mean,\n                                                 \"median\" = median,\n                                                 \"var\" = var,\n                                                 \"sd\" = sd,\n                                                 \"IQR\" = IQR), .names = \"{.fn}_{.col}\"))\n\nbike_summaries\n\n$general\n# A tibble: 1 × 5\n  mean_rented_bike_count median_rented_bike_count var_rented_bike_count\n                   &lt;dbl&gt;                    &lt;dbl&gt;                 &lt;dbl&gt;\n1                   705.                     504.               416022.\n# ℹ 2 more variables: sd_rented_bike_count &lt;dbl&gt;, IQR_rented_bike_count &lt;dbl&gt;\n\n$season\n# A tibble: 4 × 6\n  season mean_rented_bike_count median_rented_bike_count var_rented_bike_count\n  &lt;fct&gt;                   &lt;dbl&gt;                    &lt;dbl&gt;                 &lt;dbl&gt;\n1 Autumn                   820.                     764.               423912.\n2 Spring                   730.                     583                386274.\n3 Summer                  1034.                     906.               476438.\n4 Winter                   226.                     203                 22612.\n# ℹ 2 more variables: sd_rented_bike_count &lt;dbl&gt;, IQR_rented_bike_count &lt;dbl&gt;\n\n$holiday\n# A tibble: 2 × 6\n  holiday    mean_rented_bike_count median_rented_bike_c…¹ var_rented_bike_count\n  &lt;fct&gt;                       &lt;dbl&gt;                  &lt;dbl&gt;                 &lt;dbl&gt;\n1 Holiday                      500.                   240                325782.\n2 No Holiday                   715.                   524.               418453.\n# ℹ abbreviated name: ¹​median_rented_bike_count\n# ℹ 2 more variables: sd_rented_bike_count &lt;dbl&gt;, IQR_rented_bike_count &lt;dbl&gt;\n\n$func_day\n# A tibble: 2 × 6\n  func_day mean_rented_bike_count median_rented_bike_count var_rented_bike_count\n  &lt;fct&gt;                     &lt;dbl&gt;                    &lt;dbl&gt;                 &lt;dbl&gt;\n1 No                           0                         0                    0 \n2 Yes                        729.                      542               412615.\n# ℹ 2 more variables: sd_rented_bike_count &lt;dbl&gt;, IQR_rented_bike_count &lt;dbl&gt;\n\n\nOne major thing that stands out is no bikes are sold on a non-functioning day. This makes sense because a bike shop cannot sell bikes when it is closed. We will subset the data to only look at functioning days\n\nbike_data &lt;- bike_data %&gt;%\n  filter(func_day == \"Yes\")\n\nFor modeling and summaries later, we want to look at day-level granularity rather than hourly. Lets transform the data using dplyr to give us some appropriate aggregate measures of our weather related variables.\nWe’ll group by date, season and holiday.\n\nagg_bike_data &lt;- bike_data %&gt;%\n  group_by(date,season,holiday) %&gt;%\n  summarize(rented_bike_count= sum(rented_bike_count),\n            total_rainfall_mm = sum(rainfall_mm),\n            total_snowfall_cm = sum(snowfall_cm),\n            avg_temp_c = mean(temperature_c),\n            avg_humidity_perc = mean(humidity_perc),\n            avg_windspeed_ms = mean(wind_speed_ms),\n            avg_dew_temp = mean(dew_temp),\n            avg_solar_radiation = mean(solar_radiation),\n            avg_visibility = mean(visibility)\n            )\n\n`summarise()` has grouped output by 'date', 'season'. You can override using\nthe `.groups` argument.\n\nhead(agg_bike_data)\n\n# A tibble: 6 × 12\n# Groups:   date, season [6]\n  date       season holiday    rented_bike_count total_rainfall_mm\n  &lt;date&gt;     &lt;fct&gt;  &lt;fct&gt;                  &lt;dbl&gt;             &lt;dbl&gt;\n1 2017-12-01 Winter No Holiday              9539               0  \n2 2017-12-02 Winter No Holiday              8523               0  \n3 2017-12-03 Winter No Holiday              7222               4  \n4 2017-12-04 Winter No Holiday              8729               0.1\n5 2017-12-05 Winter No Holiday              8307               0  \n6 2017-12-06 Winter No Holiday              6669               1.3\n# ℹ 7 more variables: total_snowfall_cm &lt;dbl&gt;, avg_temp_c &lt;dbl&gt;,\n#   avg_humidity_perc &lt;dbl&gt;, avg_windspeed_ms &lt;dbl&gt;, avg_dew_temp &lt;dbl&gt;,\n#   avg_solar_radiation &lt;dbl&gt;, avg_visibility &lt;dbl&gt;\n\n\nLets recreate our basic summary tables from the previous steps using this data. There is no need to do this for func_day anymore since there is only one value after our previous subsetting\n\nagg_bike_summaries &lt;- list(general=NULL, season = NULL, holiday = NULL)\n\nagg_bike_summaries$season &lt;- summarizeNumeric(agg_bike_data,\"season\")\nagg_bike_summaries$holiday &lt;- summarizeNumeric(agg_bike_data,\"holiday\")\n\nagg_bike_summaries$general &lt;- agg_bike_data %&gt;%\n  select(rented_bike_count) %&gt;%\n  summarize(across(everything(), .fns = list(\"mean\" = mean,\n                                                 \"median\" = median,\n                                                 \"var\" = var,\n                                                 \"sd\" = sd,\n                                                 \"IQR\" = IQR), .names = \"{.fn}_{.col}\"))\n\nAdding missing grouping variables: `date`, `season`\n`summarise()` has grouped output by 'date'. You can override using the\n`.groups` argument.\n\nagg_bike_summaries\n\n$general\n# A tibble: 353 × 7\n# Groups:   date [353]\n   date       season mean_rented_bike_count median_rented_bike_count\n   &lt;date&gt;     &lt;fct&gt;                   &lt;dbl&gt;                    &lt;dbl&gt;\n 1 2017-12-01 Winter                   9539                     9539\n 2 2017-12-02 Winter                   8523                     8523\n 3 2017-12-03 Winter                   7222                     7222\n 4 2017-12-04 Winter                   8729                     8729\n 5 2017-12-05 Winter                   8307                     8307\n 6 2017-12-06 Winter                   6669                     6669\n 7 2017-12-07 Winter                   8549                     8549\n 8 2017-12-08 Winter                   8032                     8032\n 9 2017-12-09 Winter                   7233                     7233\n10 2017-12-10 Winter                   3453                     3453\n# ℹ 343 more rows\n# ℹ 3 more variables: var_rented_bike_count &lt;dbl&gt;, sd_rented_bike_count &lt;dbl&gt;,\n#   IQR_rented_bike_count &lt;dbl&gt;\n\n$season\n# A tibble: 4 × 6\n  season mean_rented_bike_count median_rented_bike_count var_rented_bike_count\n  &lt;fct&gt;                   &lt;dbl&gt;                    &lt;dbl&gt;                 &lt;dbl&gt;\n1 Autumn                   924.                     856                381365.\n2 Spring                   746.                     599                382750.\n3 Summer                  1034.                     906.               476438.\n4 Winter                   226.                     203                 22612.\n# ℹ 2 more variables: sd_rented_bike_count &lt;dbl&gt;, IQR_rented_bike_count &lt;dbl&gt;\n\n$holiday\n# A tibble: 2 × 6\n  holiday    mean_rented_bike_count median_rented_bike_c…¹ var_rented_bike_count\n  &lt;fct&gt;                       &lt;dbl&gt;                  &lt;dbl&gt;                 &lt;dbl&gt;\n1 Holiday                      529.                    259               329398.\n2 No Holiday                   739.                    561               414742.\n# ℹ abbreviated name: ¹​median_rented_bike_count\n# ℹ 2 more variables: sd_rented_bike_count &lt;dbl&gt;, IQR_rented_bike_count &lt;dbl&gt;\n\n\nWe want to explore some relationships we’re curious about and visualize them in plots. There are more than a dozen we can explore, but for the purpose of keeping this concise you can the following plots an their observations.\n\nScatter plot between rented bikes and the average temperature colored by season. We notice a positive correlation and obvious grouping of temperatures based on season. This is expected.\nScatter plot between rented bikes and the average solar radiation colored by season. We notice a positive correlation and obvious grouping of solar radiation based on season. This is expected.\nDensity plot for units sold colored by season. We see a larger spread for most seasons except for winter which seems to hold a smaller spread of units sold by day.\nBoxplot for visibility across season. We observe boxplots with somewhat spread, but spring seems to have a lower median that others indicating lower visibility. Perhaps this is due to fog in the spring.\n\n\nsales_temp_scatter &lt;- ggplot(agg_bike_data,aes(x=avg_temp_c,y=rented_bike_count,color=season)) +\n  geom_point() +\n  labs(title='Temp & Units Rented Plot colored by Season') +\n  xlab('Temperature (C)') +\n  ylab('Bikes Rented')\n\nsales_radiation_scatter &lt;- ggplot(agg_bike_data,aes(x=avg_solar_radiation,y=rented_bike_count,color=season)) +\n  geom_point() +\n  labs(title='Radiation & Units Rented Plot colored by Season') +\n  xlab('Radiation') +\n  ylab('Bikes Rented')\n\nseason_sales_dens &lt;-\n  ggplot(agg_bike_data,aes(x=rented_bike_count)) +\n  geom_density(aes(fill=season),alpha=0.6) +\n  labs(title = 'Density plot of Unit sales over seasons',fill = 'Season') +\n  xlab('Units Rented') +\n  ylab('Density')\n\nseason_visibility_box &lt;- ggplot(agg_bike_data, aes(x=season, y= avg_visibility)) +\n  geom_boxplot(varwidth=T, fill=\"lightblue\") + \n  labs(title=\"Visibility by Season Box\", \n       x=\"Season\",\n       y=\"Visibility\")\n  \n\nsales_temp_scatter\n\n\n\nsales_radiation_scatter\n\n\n\nseason_sales_dens\n\n\n\nseason_visibility_box\n\n\n\n\nWe want to calculate some correlations. You can read the output below as a correlation matrix. Some notable relationships include…\n\n0.75 correlation value between the bike count and the average temperature for that day\n0.735 correlation value between the bike count and the average solar radiation for that day\nWeak but negative correlation (~-0.25) for rainfall, snow and wind against bike count.\n\nAll of these loosely point to more sales on warm and sunny days!\n\nnumeric_vars &lt;- agg_bike_data %&gt;% \n  ungroup() %&gt;%\n  select(where(is.numeric))\n\ncor(numeric_vars)\n\n                    rented_bike_count total_rainfall_mm total_snowfall_cm\nrented_bike_count          1.00000000       -0.23910905       -0.26529110\ntotal_rainfall_mm         -0.23910905        1.00000000       -0.02313404\ntotal_snowfall_cm         -0.26529110       -0.02313404        1.00000000\navg_temp_c                 0.75307673        0.14451727       -0.26696366\navg_humidity_perc          0.03588697        0.52864263        0.06539191\navg_windspeed_ms          -0.19288142       -0.10167578        0.02088156\navg_dew_temp               0.65047655        0.26456621       -0.20955286\navg_solar_radiation        0.73589290       -0.32270413       -0.23343056\navg_visibility             0.16599375       -0.22199387       -0.10188902\n                      avg_temp_c avg_humidity_perc avg_windspeed_ms\nrented_bike_count    0.753076732        0.03588697      -0.19288142\ntotal_rainfall_mm    0.144517274        0.52864263      -0.10167578\ntotal_snowfall_cm   -0.266963662        0.06539191       0.02088156\navg_temp_c           1.000000000        0.40416749      -0.26072179\navg_humidity_perc    0.404167486        1.00000000      -0.23425778\navg_windspeed_ms    -0.260721792       -0.23425778       1.00000000\navg_dew_temp         0.962796255        0.63204729      -0.28770322\navg_solar_radiation  0.550274301       -0.27444967       0.09612635\navg_visibility       0.002336683       -0.55917733       0.20602264\n                    avg_dew_temp avg_solar_radiation avg_visibility\nrented_bike_count      0.6504765          0.73589290    0.165993749\ntotal_rainfall_mm      0.2645662         -0.32270413   -0.221993866\ntotal_snowfall_cm     -0.2095529         -0.23343056   -0.101889019\navg_temp_c             0.9627963          0.55027430    0.002336683\navg_humidity_perc      0.6320473         -0.27444967   -0.559177334\navg_windspeed_ms      -0.2877032          0.09612635    0.206022636\navg_dew_temp           1.0000000          0.38315713   -0.153551591\navg_solar_radiation    0.3831571          1.00000000    0.271395906\navg_visibility        -0.1535516          0.27139591    1.000000000"
  },
  {
    "objectID": "models.html#modeling",
    "href": "models.html#modeling",
    "title": "Modeling For Data Science",
    "section": "Modeling",
    "text": "Modeling\nNow that we’ve done some exploratory analysis, lets get started on our model creation. First we’ll split our data in test and training sets (seasons as strata). We’ll also split our training set in folds for cross-validation.\nWe can see our split is 75/25 (training/testing) and that there are 10 folds in our training set in the output below.\n\nbike_split &lt;- initial_split(agg_bike_data,prop=0.75,strata=season)\nbike_train &lt;- training(bike_split)\nbike_test &lt;- testing(bike_split)\nbike_train_10_fold &lt;- vfold_cv(bike_train,10)\n\nbike_split\n\n&lt;Training/Testing/Total&gt;\n&lt;263/90/353&gt;\n\nbike_train_10_fold\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits           id    \n   &lt;list&gt;           &lt;chr&gt; \n 1 &lt;split [236/27]&gt; Fold01\n 2 &lt;split [236/27]&gt; Fold02\n 3 &lt;split [236/27]&gt; Fold03\n 4 &lt;split [237/26]&gt; Fold04\n 5 &lt;split [237/26]&gt; Fold05\n 6 &lt;split [237/26]&gt; Fold06\n 7 &lt;split [237/26]&gt; Fold07\n 8 &lt;split [237/26]&gt; Fold08\n 9 &lt;split [237/26]&gt; Fold09\n10 &lt;split [237/26]&gt; Fold10\n\n\nLets construct three recipes. For each recipe, we’ll factor our dates to either “Weekday” or “Weekend” depending on the day of the week. We’ll also normalize our numeric variables and create dummy variables for our categoricals.\nHere’s where our 3 models different slightly: 1. Recipe 1 is exactly as described above with no additional changes 2. Recipe 2 adds interactions between holiday & seasons, seasons & temperature, and temperature & rainfall 3. Recipe 3 includes everything in Recipe 2 with the added complexity of our numeric predictors having quadratic terms.\n\nrecipe_1 &lt;- recipe(rented_bike_count ~ ., data = bike_train) |&gt;\n  update_role(date, new_role = \"ID\") |&gt;\n  step_date(date,features=c(\"dow\")) |&gt;\n  step_mutate(date_dow = factor(date_dow,levels=unique(date_dow),labels=if_else(unique(date_dow) %in% c('Mon','Tue','Wed','Thu','Fri'),\"Weekday\",\"Weekend\"))) |&gt;\n  step_normalize(all_numeric(), -all_outcomes()) |&gt;\n  step_dummy(season,holiday,date_dow)\n\nrecipe_2 &lt;- recipe(rented_bike_count ~ ., data = bike_train) |&gt;\n  update_role(date, new_role = \"ID\") |&gt;\n  step_date(date,features=c(\"dow\")) |&gt;\n  step_mutate(date_dow = factor(date_dow,levels=unique(date_dow),labels=if_else(unique(date_dow) %in% c('Mon','Tue','Wed','Thu','Fri'),\"Weekday\",\"Weekend\"))) |&gt;\n  step_normalize(all_numeric(), -all_outcomes()) |&gt;\n  step_dummy(season,holiday,date_dow) |&gt;\n  step_interact(terms = ~ starts_with(\"season\"):starts_with(\"holiday\") + \n                  starts_with(\"season\"):avg_temp_c +\n                  avg_temp_c:total_rainfall_mm)\n\nrecipe_3 &lt;- recipe(rented_bike_count ~ ., data = bike_train) |&gt;\n  update_role(date, new_role = \"ID\") |&gt;\n  step_date(date,features=c(\"dow\")) |&gt;\n  step_mutate(date_dow = factor(date_dow,levels=unique(date_dow),labels=if_else(unique(date_dow) %in% c('Mon','Tue','Wed','Thu','Fri'),\"Weekday\",\"Weekend\"))) |&gt;\n  step_normalize(all_numeric(), -all_outcomes()) |&gt;\n  step_poly(all_numeric_predictors(), degree = 2, keep_original_cols = FALSE) |&gt;\n  step_dummy(season,holiday,date_dow) |&gt;\n  step_interact(terms = ~ starts_with(\"season\"):starts_with(\"holiday\") + \n                  starts_with(\"season\"):avg_temp_c_poly_1 +\n                  avg_temp_c_poly_1:total_rainfall_mm_poly_1)\n\nrecipe_1\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 10\nID:         1\n\n\n\n\n\n── Operations \n\n\n• Date features from: date\n\n\n• Variable mutation for: factor(date_dow, levels = unique(date_dow), labels =\n  if_else(unique(date_dow) %in% c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\"),\n  \"Weekday\", \"Weekend\"))\n\n\n• Centering and scaling for: all_numeric() and -all_outcomes()\n\n\n• Dummy variables from: season, holiday, date_dow\n\nrecipe_2\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 10\nID:         1\n\n\n\n\n\n── Operations \n\n\n• Date features from: date\n\n\n• Variable mutation for: factor(date_dow, levels = unique(date_dow), labels =\n  if_else(unique(date_dow) %in% c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\"),\n  \"Weekday\", \"Weekend\"))\n\n\n• Centering and scaling for: all_numeric() and -all_outcomes()\n\n\n• Dummy variables from: season, holiday, date_dow\n\n\n• Interactions with: ...\n\nrecipe_3\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 10\nID:         1\n\n\n\n\n\n── Operations \n\n\n• Date features from: date\n\n\n• Variable mutation for: factor(date_dow, levels = unique(date_dow), labels =\n  if_else(unique(date_dow) %in% c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\"),\n  \"Weekday\", \"Weekend\"))\n\n\n• Centering and scaling for: all_numeric() and -all_outcomes()\n\n\n• Orthogonal polynomials on: all_numeric_predictors()\n\n\n• Dummy variables from: season, holiday, date_dow\n\n\n• Interactions with: ...\n\n\nNow that we’ve got our recipe, lets set up a linear regression model and use the “lm” engine\n\nbike_mod &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\")\n\nbike_mod\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nWe’ll use our 10 fold CV training set in our models with each recipe. Before doing this we need to create our individual workflows to collect metrics.\nLooking at our CV error (2 for each), we see that our third model (interactions & polynomials) is our best model with the lowest RMSE!\n\nbike_wfl_1 &lt;- workflow() |&gt;\n  add_recipe(recipe_1) |&gt;\n  add_model(bike_mod)\n  \nbike_fit_1 &lt;- bike_wfl_1 |&gt;\n  fit_resamples(bike_train_10_fold)\n\nbike_wfl_2 &lt;- workflow() |&gt;\n  add_recipe(recipe_2) |&gt;\n  add_model(bike_mod)\n\nbike_fit_2 &lt;- bike_wfl_2 |&gt;\n  fit_resamples(bike_train_10_fold)\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x2\n\n\n\n\nbike_wfl_3 &lt;- workflow() |&gt;\n  add_recipe(recipe_3) |&gt;\n  add_model(bike_mod)\n\nbike_fit_3 &lt;- bike_wfl_3 |&gt;\n  fit_resamples(bike_train_10_fold)\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x2\n\n\n\n\nrbind(bike_fit_1 |&gt; collect_metrics(),bike_fit_2 |&gt; collect_metrics(),bike_fit_3 |&gt; collect_metrics())\n\n# A tibble: 6 × 6\n  .metric .estimator     mean     n  std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   4060.       10 210.     Preprocessor1_Model1\n2 rsq     standard      0.832    10   0.0228 Preprocessor1_Model1\n3 rmse    standard   3353.       10 240.     Preprocessor1_Model1\n4 rsq     standard      0.877    10   0.0229 Preprocessor1_Model1\n5 rmse    standard   3309.       10 191.     Preprocessor1_Model1\n6 rsq     standard      0.883    10   0.0192 Preprocessor1_Model1\n\n\nSince our interaction and polynomial model is our best model, we want to keep this, evaluate against our entire training set and test against our test set.\nOur RMSE evaluated against our test data can be seen in the first output module. We can also see our coefficients from the fitted model in the second output module.\n\nmlr_test_metrics &lt;- bike_wfl_3 |&gt;\n  last_fit(bike_split,metrics=metric_set(rmse,mae)) |&gt;\n  collect_metrics()\n\nfinal_model &lt;- bike_wfl_3 |&gt;\n  fit(bike_train) |&gt;\n  extract_fit_parsnip() |&gt;\n  tidy()\n\nmlr_test_metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       2629. Preprocessor1_Model1\n2 mae     standard       2007. Preprocessor1_Model1\n\nfinal_model\n\n# A tibble: 29 × 5\n   term                     estimate std.error statistic       p.value\n   &lt;chr&gt;                       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n 1 (Intercept)                14157.     2312.     6.12  0.00000000384\n 2 total_rainfall_mm_poly_1  -23264.     7799.    -2.98  0.00316      \n 3 total_rainfall_mm_poly_2   14515.     3703.     3.92  0.000116     \n 4 total_snowfall_cm_poly_1    -846.     3501.    -0.242 0.809        \n 5 total_snowfall_cm_poly_2   -2099.     3167.    -0.663 0.508        \n 6 avg_temp_c_poly_1         -40134.    79516.    -0.505 0.614        \n 7 avg_temp_c_poly_2         -23101.    19064.    -1.21  0.227        \n 8 avg_humidity_perc_poly_1  -46149.    27239.    -1.69  0.0915       \n 9 avg_humidity_perc_poly_2   -9666.     6515.    -1.48  0.139        \n10 avg_windspeed_ms_poly_1    -7441.     3474.    -2.14  0.0332       \n# ℹ 19 more rows\n\n\n\nMLR Summary\nIn our output above, we see a table of our estimates for our different parameters, which includes some interactions and polynomials. We can also see our standard errors.\nTo use this module, we can plug in our parameters and use the predict() function to see how our best MLR model would perform against new data. We won’t do this here, but we now know our coefficients."
  },
  {
    "objectID": "models.html#lasso-model",
    "href": "models.html#lasso-model",
    "title": "Modeling For Data Science",
    "section": "Lasso Model",
    "text": "Lasso Model\nStarting with our Lasso Model. We can re-use our first from the MLR steps which is simply named recipe_1. We will have our lasso model and then create our workflow.\n\nlasso_mod &lt;- linear_reg(penalty = tune(), mixture = 1) |&gt;\n  set_engine(\"glmnet\")\n\nlasso_wfl_1 &lt;- workflow() |&gt;\n  add_recipe(recipe_1) |&gt;\n  add_model(lasso_mod)\n\nWe’ll create tuning grids to execute our workflow on 200 different levels of our tuning parameter. Our CV folds will be passed for sampling.\nWe are showing the output for lasso_grid_1. We can see there are nested tibbles with metrics on the 200 models trained for each fold!\n\nlasso_grid_1 &lt;- lasso_wfl_1 |&gt;\n  tune_grid(resamples = bike_train_10_fold,\n            grid = grid_regular(penalty(), levels = 200),\n            metrics = metric_set(rmse,mae))\n\nlasso_grid_1\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits           id     .metrics           .notes          \n   &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;             &lt;list&gt;          \n 1 &lt;split [236/27]&gt; Fold01 &lt;tibble [400 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [236/27]&gt; Fold02 &lt;tibble [400 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [236/27]&gt; Fold03 &lt;tibble [400 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [237/26]&gt; Fold04 &lt;tibble [400 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [237/26]&gt; Fold05 &lt;tibble [400 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [237/26]&gt; Fold06 &lt;tibble [400 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [237/26]&gt; Fold07 &lt;tibble [400 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [237/26]&gt; Fold08 &lt;tibble [400 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [237/26]&gt; Fold09 &lt;tibble [400 × 5]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [237/26]&gt; Fold10 &lt;tibble [400 × 5]&gt; &lt;tibble [0 × 3]&gt;\n\n\nGoing one step further, lets unpack the above output to make it a little cleaner and more readable. To reduce clutter, we’ll show the output for lasso_grid_1.\nAgain, this seems hard to read as the RMSE seems static across penalties. Note that there are 200 records in this tibble, so we should see some differences on the back-end of our records.\n\nlasso_grid_1 |&gt;\n  collect_metrics() |&gt;\n  filter(.metric %in% c(\"rmse\"))\n\n# A tibble: 200 × 7\n    penalty .metric .estimator  mean     n std_err .config               \n      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n 1 1   e-10 rmse    standard   4066.    10    207. Preprocessor1_Model001\n 2 1.12e-10 rmse    standard   4066.    10    207. Preprocessor1_Model002\n 3 1.26e-10 rmse    standard   4066.    10    207. Preprocessor1_Model003\n 4 1.41e-10 rmse    standard   4066.    10    207. Preprocessor1_Model004\n 5 1.59e-10 rmse    standard   4066.    10    207. Preprocessor1_Model005\n 6 1.78e-10 rmse    standard   4066.    10    207. Preprocessor1_Model006\n 7 2.00e-10 rmse    standard   4066.    10    207. Preprocessor1_Model007\n 8 2.25e-10 rmse    standard   4066.    10    207. Preprocessor1_Model008\n 9 2.52e-10 rmse    standard   4066.    10    207. Preprocessor1_Model009\n10 2.83e-10 rmse    standard   4066.    10    207. Preprocessor1_Model010\n# ℹ 190 more rows\n\n\nLets fetch the lowest RMSE for each of our workflows and showcase the output. We see our tuned penalty parameter below.\n\nlowest_rmse_1 &lt;- lasso_grid_1 |&gt;\n  select_best(metric = \"rmse\")\n\nrbind(lowest_rmse_1)\n\n# A tibble: 1 × 2\n       penalty .config               \n         &lt;dbl&gt; &lt;chr&gt;                 \n1 0.0000000001 Preprocessor1_Model001\n\n\nUsing our best model, we’ll train on the training dataset and evaluate performance against the test set. See our metrics below for RMSE for our best Lasso Model.\n\nlasso_final &lt;- lasso_wfl_1 |&gt;\n  finalize_workflow(lowest_rmse_1) |&gt;\n  last_fit(bike_split,metrics=metric_set(rmse,mae))\n\nlasso_test_metrics &lt;- lasso_final |&gt;\n  collect_metrics()\n\nlasso_test_metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       4258. Preprocessor1_Model1\n2 mae     standard       3261. Preprocessor1_Model1\n\n\n\nLasso Summary\nBelow we see our table of coefficients for our best lasso model fit to our training data. This includes our estimates and our tuning parameter (penalty)\n\nlasso_summary &lt;- lasso_wfl_1 |&gt;\n  finalize_workflow(lowest_rmse_1) |&gt;\n  fit(bike_train)\ntidy(lasso_summary)\n\n# A tibble: 14 × 3\n   term                estimate      penalty\n   &lt;chr&gt;                  &lt;dbl&gt;        &lt;dbl&gt;\n 1 (Intercept)          16736.  0.0000000001\n 2 total_rainfall_mm    -1972.  0.0000000001\n 3 total_snowfall_cm     -177.  0.0000000001\n 4 avg_temp_c               0   0.0000000001\n 5 avg_humidity_perc     -820.  0.0000000001\n 6 avg_windspeed_ms      -856.  0.0000000001\n 7 avg_dew_temp          3937.  0.0000000001\n 8 avg_solar_radiation   4452.  0.0000000001\n 9 avg_visibility          92.6 0.0000000001\n10 season_Spring        -5289.  0.0000000001\n11 season_Summer        -3834.  0.0000000001\n12 season_Winter        -8106.  0.0000000001\n13 holiday_No.Holiday    3346.  0.0000000001\n14 date_dow_Weekday      2396.  0.0000000001"
  },
  {
    "objectID": "models.html#regression-tree",
    "href": "models.html#regression-tree",
    "title": "Modeling For Data Science",
    "section": "Regression Tree",
    "text": "Regression Tree\nNow for our regression tree model. We’ll assume we are using recipe_1 from previous. Lets define our model and store our regression tree workflow.\nWe are tuning on the depth of our tree and cost complexity with the minimum number of nodes being 20.\n\ntree_mod &lt;- decision_tree(tree_depth = tune(),\n                          min_n = 20,\n                          cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\ntree_wfl &lt;- workflow() |&gt;\n  add_recipe(recipe_1) |&gt;\n  add_model(tree_mod)\n\ntree_wfl\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_normalize()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (regression)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n  min_n = 20\n\nComputational engine: rpart \n\n\nNow we want to fit our data to our cross-validation folds. We’ll use our workflow we created above and tune our grid using the folds. Note that we are not specifying the levels for tuning complexity and tree depth.\nWe can see in our output the cost complexity and tree depth parameters used along with their RMSE and RSQ.\n\ntree_fits &lt;- tree_wfl |&gt; \n  tune_grid(resamples = bike_train_10_fold)\n\ntree_fits |&gt;\n  collect_metrics()\n\n# A tibble: 20 × 8\n   cost_complexity tree_depth .metric .estimator     mean     n  std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;  \n 1        2.03e- 3         11 rmse    standard   3807.       10 137.     Prepro…\n 2        2.03e- 3         11 rsq     standard      0.843    10   0.0215 Prepro…\n 3        2.92e- 4         10 rmse    standard   3799.       10 146.     Prepro…\n 4        2.92e- 4         10 rsq     standard      0.844    10   0.0219 Prepro…\n 5        1.20e-10          8 rmse    standard   3817.       10 144.     Prepro…\n 6        1.20e-10          8 rsq     standard      0.842    10   0.0219 Prepro…\n 7        7.47e- 5          2 rmse    standard   4698.       10 279.     Prepro…\n 8        7.47e- 5          2 rsq     standard      0.764    10   0.0355 Prepro…\n 9        5.85e- 2          1 rmse    standard   6965.       10 374.     Prepro…\n10        5.85e- 2          1 rsq     standard      0.512    10   0.0522 Prepro…\n11        1.42e- 5         13 rmse    standard   3798.       10 146.     Prepro…\n12        1.42e- 5         13 rsq     standard      0.844    10   0.0219 Prepro…\n13        4.08e- 8          5 rmse    standard   4058.       10 160.     Prepro…\n14        4.08e- 8          5 rsq     standard      0.826    10   0.0239 Prepro…\n15        3.09e- 7          9 rmse    standard   3801.       10 146.     Prepro…\n16        3.09e- 7          9 rsq     standard      0.844    10   0.0218 Prepro…\n17        3.68e- 9          6 rmse    standard   3878.       10 141.     Prepro…\n18        3.68e- 9          6 rsq     standard      0.838    10   0.0218 Prepro…\n19        8.97e- 7         14 rmse    standard   3798.       10 146.     Prepro…\n20        8.97e- 7         14 rsq     standard      0.844    10   0.0219 Prepro…\n\n\nWhich one is our best? Well we see that the parameters that give us the lowest RMSE have the below cost complexity and 11 layers in the tree (depth)\n\ntree_best_params &lt;- select_best(tree_fits, metric = \"rmse\")\n\ntree_best_params\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1       0.0000142         13 Preprocessor1_Model06\n\n\nUsing the above, lets finalize our workflow using our best parameters. Using this workflow we’ll train on our training set and calculate our performance metrics on the test set.\nWe see our RMSE and RSQ of our best regression tree model outputted below.\n\ntree_final_wfl &lt;- tree_wfl |&gt;\n  finalize_workflow(tree_best_params)\n\ntree_final_fit &lt;- tree_final_wfl |&gt;\n  last_fit(bike_split,metrics=metric_set(rmse,mae))\n\ntree_test_metrics &lt;- tree_final_fit |&gt;\n  collect_metrics()\n\ntree_test_metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3670. Preprocessor1_Model1\n2 mae     standard       2697. Preprocessor1_Model1\n\n\n\nRegression Tree Summary\nIn the below graph, we can see how our regression tree is working in a graph.\n\ntree_final_fit |&gt;\n  extract_workflow(tree_final_fit) |&gt;\n  extract_fit_engine() |&gt;\n  rpart.plot::rpart.plot(roundint = FALSE)"
  },
  {
    "objectID": "models.html#bagged-tree",
    "href": "models.html#bagged-tree",
    "title": "Modeling For Data Science",
    "section": "Bagged Tree",
    "text": "Bagged Tree\nNow we’ll tune a bag tree model and pick the best from this class of models. Per usual, lets start with defining the model and building the workflow. Again, we’ll use recipe_3 from our linear regression section since we can reuse it here.\nOur tree depth is set to 5 and the minimum number of nodes is 10 leaving cost complexity as our tunning parameter.\n\nbag_model &lt;- bag_tree(tree_depth = 5, min_n = 10, cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\nbag_wfl &lt;- workflow() |&gt;\n  add_recipe(recipe_1) |&gt;\n  add_model(bag_model)\n\nbag_wfl\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: bag_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_normalize()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nBagged Decision Tree Model Specification (regression)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = 5\n  min_n = 10\n\nComputational engine: rpart \n\n\nNow to fit our model. We’ll use our cross-validation folds and ensure the metrics return are accuracy and log-loss.\nWe see our best fit model on the first tow of the output above.\n\nbag_fit &lt;- bag_wfl |&gt;\n  tune_grid(resamples = bike_train_10_fold,\n            grid = grid_regular(cost_complexity(),\n                                levels = 15),\n            metrics = metric_set(rmse))\n\n\nbag_fit |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\")\n\n# A tibble: 15 × 7\n   cost_complexity .metric .estimator  mean     n std_err .config              \n             &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1        1   e-10 rmse    standard   3221.    10    203. Preprocessor1_Model01\n 2        4.39e-10 rmse    standard   3202.    10    168. Preprocessor1_Model02\n 3        1.93e- 9 rmse    standard   3192.    10    185. Preprocessor1_Model03\n 4        8.48e- 9 rmse    standard   3147.    10    174. Preprocessor1_Model04\n 5        3.73e- 8 rmse    standard   3131.    10    221. Preprocessor1_Model05\n 6        1.64e- 7 rmse    standard   3184.    10    154. Preprocessor1_Model06\n 7        7.20e- 7 rmse    standard   3126.    10    200. Preprocessor1_Model07\n 8        3.16e- 6 rmse    standard   3177.    10    185. Preprocessor1_Model08\n 9        1.39e- 5 rmse    standard   3127.    10    186. Preprocessor1_Model09\n10        6.11e- 5 rmse    standard   3061.    10    207. Preprocessor1_Model10\n11        2.68e- 4 rmse    standard   3236.    10    142. Preprocessor1_Model11\n12        1.18e- 3 rmse    standard   3152.    10    193. Preprocessor1_Model12\n13        5.18e- 3 rmse    standard   3190.    10    187. Preprocessor1_Model13\n14        2.28e- 2 rmse    standard   3762.    10    197. Preprocessor1_Model14\n15        1   e- 1 rmse    standard   4786.    10    318. Preprocessor1_Model15\n\n\nUsing that model with our tuned parameters, we’ll finalize our workflow, train the data and look at performance metrics on the test set. We’ll use the lowest RMSE to select our best model\n\nbag_best_params &lt;- select_best(bag_fit, metric = \"rmse\")\n\nbag_final_wkf &lt;- bag_wfl |&gt;\n  finalize_workflow(bag_best_params)\n\nbag_final_fit &lt;- bag_final_wkf |&gt;\n  last_fit(bike_split, metrics = metric_set(rmse,mae))\n\nbag_test_metrics &lt;- bag_final_fit |&gt;\n  collect_metrics()\n\nbag_test_metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       3059. Preprocessor1_Model1\n2 mae     standard       2341. Preprocessor1_Model1\n\n\n\nBagged Summary\nBelow we can see our final variable plot for our bagged tree. This tells us the importance of our variables. We see that Average Temperature, Average Solar Radiation and Average Dew Temp are our most important variables.\n\nbag_extract_fit &lt;- bag_final_wkf |&gt;\n  fit(bike_train) |&gt;\n  extract_fit_engine()\n\nbag_extract_fit$imp |&gt;\n  mutate(term = factor(term, levels = term)) |&gt;\n  ggplot(aes(x = term, y = value)) +\n  geom_bar(stat =\"identity\") +\n  coord_flip()"
  },
  {
    "objectID": "models.html#random-forest",
    "href": "models.html#random-forest",
    "title": "Modeling For Data Science",
    "section": "Random Forest",
    "text": "Random Forest\nThe last model we want to evaluate is a random forest model. Time for our model and workflow setup!\n\nrf_model &lt;- rand_forest(mtry = tune()) |&gt;\n  set_engine(\"ranger\", importance = \"impurity\") |&gt;\n  set_mode(\"regression\")\n\nrf_wfl &lt;- workflow() |&gt;\n  add_recipe(recipe_1) |&gt;\n  add_model(rf_model)\n\nrf_model\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n\nEngine-Specific Arguments:\n  importance = impurity\n\nComputational engine: ranger \n\n\nLets fit it to our CV folds.\n\nrf_fit &lt;- rf_wfl |&gt;\n  tune_grid(resamples = bike_train_10_fold,\n            grid = 7,\n            metrics = metric_set(rmse))\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\nrf_fit |&gt;\ncollect_metrics() |&gt;\nfilter(.metric == \"rmse\") |&gt;\narrange(mean)\n\n# A tibble: 6 × 7\n   mtry .metric .estimator  mean     n std_err .config             \n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1     9 rmse    standard   2767.    10    175. Preprocessor1_Model5\n2    11 rmse    standard   2791.    10    183. Preprocessor1_Model3\n3    12 rmse    standard   2796.    10    176. Preprocessor1_Model4\n4     7 rmse    standard   2838.    10    181. Preprocessor1_Model2\n5     4 rmse    standard   2971.    10    169. Preprocessor1_Model6\n6     2 rmse    standard   3450.    10    145. Preprocessor1_Model1\n\n\nPlaceholder after issue resolved on all models failing\n\nrf_best_params &lt;- select_best(rf_fit, metric=\"rmse\")\n\nrf_final_wfl &lt;- rf_wfl |&gt;\n  finalize_workflow(rf_best_params)\n\nrf_final_fit &lt;- rf_final_wfl |&gt;\n  last_fit(bike_split, metrics = metric_set(rmse,mae))\n\nrf_test_metrics &lt;- rf_final_fit |&gt;\n  collect_metrics()\n\nrf_test_metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       2733. Preprocessor1_Model1\n2 mae     standard       2154. Preprocessor1_Model1\n\n\n\nRandom Forest Summary\nIn our fit on our bike train data, we see that 2 of our top 3 variables in terms of importance are the same as our Bagged Model. Those two are Average Temperature and Average Solar Radiation. Where it differs is the binary variable denoting the season winter as our third most important variable in this model!\n\nrf_extract_fit &lt;- rf_final_wfl |&gt;\n  fit(bike_train) |&gt;\n  extract_fit_engine()\n\nrf_extract_fit |&gt;\n  vip::vi() |&gt;\n  arrange(Importance) |&gt;\n  ggplot(aes(x = Variable, y = Importance)) +\n  geom_bar(stat =\"identity\") +\n  coord_flip()"
  },
  {
    "objectID": "models.html#comparing-all-our-models",
    "href": "models.html#comparing-all-our-models",
    "title": "Modeling For Data Science",
    "section": "Comparing all our models",
    "text": "Comparing all our models\nLets look at all our models. We want to pick the best model from the RMSE statistic and the MAE statistic.\nWe can see for both MAE and RMSE, our Multiple Linear regression model is the winner (last two rows) and boast the lowest values for each!\n\nrbind(rf_test_metrics,bag_test_metrics,lasso_test_metrics,tree_test_metrics,mlr_test_metrics)\n\n# A tibble: 10 × 4\n   .metric .estimator .estimate .config             \n   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n 1 rmse    standard       2733. Preprocessor1_Model1\n 2 mae     standard       2154. Preprocessor1_Model1\n 3 rmse    standard       3059. Preprocessor1_Model1\n 4 mae     standard       2341. Preprocessor1_Model1\n 5 rmse    standard       4258. Preprocessor1_Model1\n 6 mae     standard       3261. Preprocessor1_Model1\n 7 rmse    standard       3670. Preprocessor1_Model1\n 8 mae     standard       2697. Preprocessor1_Model1\n 9 rmse    standard       2629. Preprocessor1_Model1\n10 mae     standard       2007. Preprocessor1_Model1"
  },
  {
    "objectID": "models.html#final-model",
    "href": "models.html#final-model",
    "title": "Modeling For Data Science",
    "section": "Final Model",
    "text": "Final Model\nOur MLR model was our winner! The last thing to do is to fit that model on the ENTIRE data set instead of our training and test splits. The final output and metrics are below.\nWe can see our coefficient estimates fitted against the entire dataset below for our best MLR model. This includes interactions and polynomial terms\n\nbike_wfl_3 |&gt;\n  fit(agg_bike_data) |&gt;\n  extract_fit_parsnip() |&gt;\n  tidy()\n\n# A tibble: 29 × 5\n   term                     estimate std.error statistic  p.value\n   &lt;chr&gt;                       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)                16237.     1569.    10.4   6.95e-22\n 2 total_rainfall_mm_poly_1  -31817.     6881.    -4.62  5.45e- 6\n 3 total_rainfall_mm_poly_2   15915.     3455.     4.61  5.88e- 6\n 4 total_snowfall_cm_poly_1   -3263.     3279.    -0.995 3.20e- 1\n 5 total_snowfall_cm_poly_2   -1961.     3071.    -0.639 5.24e- 1\n 6 avg_temp_c_poly_1          53669.    69530.     0.772 4.41e- 1\n 7 avg_temp_c_poly_2         -19838.    17177.    -1.15  2.49e- 1\n 8 avg_humidity_perc_poly_1  -23082.    24481.    -0.943 3.46e- 1\n 9 avg_humidity_perc_poly_2   -8217.     6120.    -1.34  1.80e- 1\n10 avg_windspeed_ms_poly_1    -8602.     3302.    -2.61  9.61e- 3\n# ℹ 19 more rows"
  }
]